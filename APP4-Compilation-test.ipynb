{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e03b51",
   "metadata": {},
   "source": [
    "# APP4 - Projet Compilateur \n",
    "\n",
    "Binôme : Nino</br>\n",
    "Langage : Python</br>\n",
    "Evalution : Rapport à rendre, code lisible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17458d35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf9a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a03d55",
   "metadata": {},
   "source": [
    "## Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_token = None\n",
    "last_token = None\n",
    "tree = None\n",
    "\n",
    "OP = {\n",
    "    \"tok_=\" : { \"prio\" : 1, \"parg\" : 1, \"node_type\" : \"nd_affect\"},\n",
    "    \"tok_||\" : { \"prio\" : 2, \"parg\" : 3, \"node_type\" : \"nd_or\"},\n",
    "    \"tok_&&\" : { \"prio\" : 3, \"parg\" : 4, \"node_type\" : \"nd_and\"},\n",
    "    \"tok_==\" : { \"prio\" : 4, \"parg\" : 5, \"node_type\" : \"nd_iseq\"},\n",
    "    \"tok_!=\" : { \"prio\" : 4, \"parg\" : 5, \"node_type\" : \"nd_isnoteq\"},\n",
    "    \"tok_<=\" : { \"prio\" : 5, \"parg\" : 6, \"node_type\" : \"nd_isinfeq\"},\n",
    "    \"tok_>=\" : { \"prio\" : 5, \"parg\" : 6, \"node_type\" : \"nd_issupeq\"},\n",
    "    \"tok_<\" : { \"prio\" : 5, \"parg\" : 6, \"node_type\" : \"nd_isinf\"},\n",
    "    \"tok_>\" : { \"prio\" : 5, \"parg\" : 6, \"node_type\" : \"nd_issup\"},\n",
    "    \"tok_+\" : { \"prio\" : 6, \"parg\" : 7, \"node_type\" : \"nd_plus\"},\n",
    "    \"tok_-\" : { \"prio\" : 6, \"parg\" : 7, \"node_type\" : \"nd_minus\"},\n",
    "    \"tok_*\" : { \"prio\" : 7, \"parg\" : 8, \"node_type\" : \"nd_mult\"},\n",
    "    \"tok_/\" : { \"prio\" : 7, \"parg\" : 8, \"node_type\" : \"nd_div\"},\n",
    "    \"tok_%\" : { \"prio\" : 7, \"parg\" : 8, \"node_type\" : \"nd_mod\"},\n",
    "}\n",
    "\n",
    "NF = {\n",
    "    # Operateurs binaires\n",
    "    \"nd_or\" : {\"prefix\" : \"\",\"suffix\" : \"or\"},\n",
    "    \"nd_and\" : {\"prefix\" : \"\",\"suffix\" : \"and\"},\n",
    "    \"nd_iseq\" : {\"prefix\" : \"\",\"suffix\" : \"cmpeq\"},\n",
    "    \"nd_isnoteq\" : {\"prefix\" : \"\",\"suffix\" : \"cmpne\"},\n",
    "    \"nd_isinfeq\" : {\"prefix\" : \"\",\"suffix\" : \"cmple\"},\n",
    "    \"nd_issupeq\" : {\"prefix\" : \"\",\"suffix\" : \"cmpge\"},\n",
    "    \"nd_isinf\" : {\"prefix\" : \"\",\"suffix\" : \"cmplt\"},\n",
    "    \"nd_issup\" : {\"prefix\" : \"\",\"suffix\" : \"cmpgt\"},\n",
    "    \"nd_plus\" : {\"prefix\" : \"\",\"suffix\" : \"add\"},\n",
    "    \"nd_minus\" : {\"prefix\" : \"\",\"suffix\" : \"sub\"},\n",
    "    \"nd_mult\" : {\"prefix\" : \"\",\"suffix\" : \"mul\"},\n",
    "    \"nd_div\" : {\"prefix\" : \"\",\"suffix\" : \"div\"},\n",
    "    \"nd_mod\" : {\"prefix\" : \"\",\"suffix\" : \"mod\"},\n",
    "\n",
    "    # Operateurs unaires\n",
    "    \"nd_not\" : {\"prefix\" : \"\", \"suffix\" : \"not\"},\n",
    "    \"nd_neg\" : {\"prefix\" : \"push 0\", \"suffix\" : \"sub\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fcaa3",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gennode(node:Node,file):\n",
    "    \"\"\" \n",
    "    Fonction générant le code associé à un noeud ou arbre de noeud\n",
    "\n",
    "    @params:\n",
    "    Entrée :    node, type : Node, Noeud dont il faut extraire le code\n",
    "                file, type : File, code à compiler\n",
    "    Sortie : None\n",
    "    \"\"\"\n",
    "\n",
    "    # Récupération du type de noeud du noeud en argument\n",
    "    node_type = node.node_type\n",
    "\n",
    "    # Si ce type de noeud est dans la table NF\n",
    "    if node_type in NF:\n",
    "\n",
    "        # Récupération de son préfixe\n",
    "        prefix = NF[node_type][\"prefix\"]\n",
    "\n",
    "        # S'il existe, affichage du préfixe\n",
    "        if prefix: print(prefix, file=file)\n",
    "\n",
    "        # Appel récursif de la fonction sur chacun des enfants du noeud\n",
    "        for child in node.children:\n",
    "            gennode(child,file=file)\n",
    "        \n",
    "        print(NF[node_type][\"suffix\"],file=file)\n",
    "        return\n",
    "\n",
    "    # Si le noeud n'est pas dans la table NF, parcours en fonction de son type\n",
    "    match node_type:\n",
    "\n",
    "        # Cas d'un noeud constant\n",
    "        case \"nd_const\":\n",
    "            print(f\"push {node.node_value}\",file=file)\n",
    "        \n",
    "        # Autres cas, renvoi d'une erreur\n",
    "        case other:\n",
    "            raise ValueError(f\"node_type {other} at pos {node.node_pos} unknown\")\n",
    "        \n",
    "def gencode(optimizer:Optimizer,file):\n",
    "    tree = optimizer.next_tree()\n",
    "    gennode(tree,file)\n",
    "\n",
    "def main():\n",
    "    with open(\"code.c\", 'r') as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    lexer = Lexer(code)\n",
    "\n",
    "def throw_error(message:str):\n",
    "    print(message)\n",
    "    quit(1)\n",
    "\n",
    "def check_op_prio(token_type : str,prio : int) -> bool:\n",
    "    return token_type in OP.keys() and OP[token_type][\"prio\"] >= prio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbeecc",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b172e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token_type:str, token_pos:Tuple[int,int], token_value:Optional[int]=None, token_string:Optional[str]=None):\n",
    "        \"\"\" \n",
    "        Classe représentant les objets Token utilisés par la suite\n",
    "\n",
    "        @params\n",
    "        Entrée : t_type,    type String, le type du token\n",
    "                 t_value,   type Int,    la valeur du token lorsqu'il représente une constante numérique (optionnel)\n",
    "                 t_string,  type String, la valeur du token lorsqu'il représente une contante alphabétique ou une variable (optionnel)\n",
    "                 t_pos,     type Tuple(ligne:int, colonne:int), la position du token dans le code\n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        assert type(token_type)==str, \"Mauvais type d'argument node_type (str attendu)\"\n",
    "        assert type(token_pos)==tuple, \"Mauvais type d'argument node_pos (tuple attendu)\"\n",
    "        for e in token_pos:\n",
    "            assert type(e)==int, f\"Mauvais type d'argument node_pose à l'indice pour {e} (int attendu)\"\n",
    "        if token_value:\n",
    "            assert type(token_value)==int, \"Mauvais type d'argument node_value (int attendu)\"\n",
    "        if token_string:\n",
    "            assert type(token_string)==str, \"Mauvais type d'argument node_string (str attendu)\"\n",
    "        \n",
    "        self.token_type = token_type\n",
    "        self.token_value = token_value\n",
    "        self.token_string = token_string\n",
    "        self.token_pos = token_pos\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Méthode d'affichage de l'object\n",
    "\n",
    "        @params:\n",
    "        Entrée : None\n",
    "        Sortie :None\n",
    "        \"\"\"\n",
    "        string = self.token_type + f\" at ({self.token_pos})\"\n",
    "\n",
    "        if self.token_value:\n",
    "            string += f\" with value : {self.token_value}\"\n",
    "        if self.token_string:\n",
    "            string += f\" with string : {self.token_string}\"\n",
    "    \n",
    "        return string\n",
    "\n",
    "\n",
    "class Node:\n",
    "     \n",
    "    def __init__(self, node_type:str,node_pos: Tuple[int,int], node_value : Optional[int] = None, node_string: Optional[str] = None, children: List['Node'] = []):\n",
    "        \"\"\" \n",
    "        Classe représentant les objets Noeud (Node) pour la construction d'arbre dans le but de gérer le code dans le bon ordre\n",
    "\n",
    "        @params\n",
    "        Entrée : node_type,     type String, type du noeud\n",
    "                 node_pos,      type Tuple(ligne:int, colonne:int), position du noeud dans le code\n",
    "                 node_value,    type Int, valeur du noeud quand c'est une constante numérique (optionnel)\n",
    "                 node_string,   type String, valeur du noeud quand c'est une constante alphabétique (optionnel)\n",
    "                 children,      type List(Node,Node...), liste des enfants du noeud\n",
    "        \"\"\"\n",
    "        assert type(node_type)==str, \"Mauvais type d'argument node_type (str attendu)\"\n",
    "        assert type(node_pos)==tuple, \"Mauvais type d'argument node_pos (tuple attendu)\"\n",
    "        for e in node_pos:\n",
    "            assert type(e)==int, f\"Mauvais type d'argument node_pose à l'indice pour {e} (int attendu)\"\n",
    "        if node_value:\n",
    "            assert type(node_value)==int, \"Mauvais type d'argument node_value (int attendu)\"\n",
    "        if node_string:\n",
    "            assert type(node_string)==str, \"Mauvais type d'argument node_string (str attendu)\"\n",
    "        assert type(children)==list, \"Mauvais type d'argument children (list attendu)\"\n",
    "        \n",
    "        self.node_type = node_type\n",
    "        self.node_pos = node_pos\n",
    "        self.node_value = node_value\n",
    "        self.node_string = node_string\n",
    "        self.children = children\n",
    "\n",
    "\n",
    "class Lexer:\n",
    "    def __init__(self, text:str):\n",
    "        \"\"\" \n",
    "        Méthode d'initialisation de la classe\n",
    "\n",
    "        @params\n",
    "        Entrée : text, type String, code à compiler\n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        assert type(text)==str, \"Mauvais type d'argument text (str attendu)\"\n",
    "\n",
    "        self.pointer_pos : int = 0\n",
    "        self.current_line : int = 0\n",
    "        self.current_col : int = 0\n",
    "\n",
    "        self.current_token = Token(\"tok_start\",(-1,-1))\n",
    "        self.last_token = Token(\"tok_start\",(-1,-1))\n",
    "\n",
    "        self.spacing_chars = [' ', '\\t', '\\n']\n",
    "\n",
    "        self.keywords = [\n",
    "            \"int\",\n",
    "            \"void\",\n",
    "            \"return\",\n",
    "            \"if\",\n",
    "            \"else\",\n",
    "            \"for\",\n",
    "            \"do\",\n",
    "            \"while\",\n",
    "            \"break\",\n",
    "            \"continue\",\n",
    "            \"debug\",\n",
    "            \"send\",\n",
    "            \"rec\"\n",
    "        ]\n",
    "\n",
    "        self.text = text\n",
    "    \n",
    "    # -----\n",
    "    # Vérification de position du curseur\n",
    "    # -----\n",
    "    def test_eof(self):\n",
    "        \"\"\" \n",
    "        Renvoi de l'égalité entre la position du pointeur parcourant le code \n",
    "        et la longueur du code\n",
    "\n",
    "        S'ils sont égaux, alors le code a fini d'être parcouru\n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        return self.pointer_pos == len(self.text)\n",
    "            \n",
    "    def eof_tok(self):\n",
    "        \"\"\" \n",
    "        Création du token 'End Of File', signifiant que le curseur est après le \n",
    "        dernier caratère du code, il a donc fini d'être parcouru\n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : Token, le token eof\n",
    "        \"\"\"\n",
    "        return Token(\"tok_eof\",(self.current_line,self.current_col))\n",
    "    \n",
    "    # -----\n",
    "    # Vérification du type de caratère rencontré\n",
    "    # -----\n",
    "    def is_number(self, character:str):\n",
    "        \"\"\"\n",
    "        Renvoie si le caractère étudié est un chiffre ou non en utilisant la table ASCII\n",
    "        \n",
    "        @params\n",
    "        Entrée : character, type string, caractère du code rencontré\n",
    "        Sortie : Boolean, résultat de la comparaison\n",
    "        \"\"\"\n",
    "        assert type(character)==str, \"Mauvais type d'argument character (str attendu)\"\n",
    "\n",
    "        return ord('0' ) <= ord(character) <= ord('9')\n",
    "    \n",
    "    def is_letter(self,character:str):\n",
    "        \"\"\"\n",
    "        Renvoie si le caractère étudié est une lettre ou non en utilisant la table ASCII\n",
    "        \n",
    "        @params\n",
    "        Entrée : character, type string, caractère du code rencontré\n",
    "        Sortie : Boolean, résultat de la comparaison\n",
    "        \"\"\"\n",
    "        assert type(character)==str, \"Mauvais type d'argument character (str attendu)\"\n",
    "\n",
    "        return (ord('A') <= ord(character) <= ord('Z')) or (ord('a') <= ord(character) <= ord('z'))\n",
    "    \n",
    "    def is_alpha_num(self,character:str):\n",
    "        \"\"\"\n",
    "        Renvoie si le caractère étudié est alphanumérique la méthode is_number et is_letter\n",
    "        \n",
    "        @params\n",
    "        Entrée : character, type string, caractère du code rencontré\n",
    "        Sortie : Boolean, résultat de la comparaison\n",
    "        \"\"\"\n",
    "        assert type(character)==str, \"Mauvais type d'argument character (str attendu)\"\n",
    "\n",
    "        return self.is_number(character) or self.is_letter(character)\n",
    "    \n",
    "    # -----\n",
    "    # Parcours du code dans le but de trouver le token suivant\n",
    "    # -----\n",
    "\n",
    "    def next_token(self):\n",
    "        \"\"\"\n",
    "        La fonction next_token recherche dans le code à compiler le prochain token en faisant\n",
    "        avancer le pointeur le parcourant.\n",
    "\n",
    "        La fonction commence par mettre à jour les token, passe les différents espace puis\n",
    "        convertit les caratères rencontrés en token.\n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Actualisation des tokens\n",
    "        self.last_token = self.current_token\n",
    "\n",
    "        # Vérification de la position du curseur par rapport à la fin du code\n",
    "        if self.test_eof():\n",
    "            # Si le code a été entièrement parcouru, le token passe en end of file\n",
    "            self.current_token = self.eof_tok()\n",
    "            # Arret de la méthode\n",
    "            return\n",
    "\n",
    "        # ---\n",
    "        # Gestion des espaces et retour à la ligne\n",
    "        # ---\n",
    "\n",
    "        # TODO: ajouter la gestion des commentaires\n",
    "\n",
    "        while (self.text[self.pointer_pos] in self.spacing_chars): # skipping spaces\n",
    "            \n",
    "            # Augmentation de l'indice à regarder\n",
    "            self.pointer_pos += 1\n",
    "\n",
    "            # S'il a dépassé la fin du code, token end of file et arret\n",
    "            if self.test_eof():\n",
    "                self.current_token = self.eof_tok()\n",
    "\n",
    "                # Une fois le token eof créé, on sort de la fonction\n",
    "                return\n",
    "\n",
    "            # Mise à jour de la position du curseur\n",
    "            self.current_col += 1\n",
    "            if self.text[self.pointer_pos] == '\\n':\n",
    "                self.current_col = 0\n",
    "                self.current_line+=1\n",
    "        \n",
    "        # ---\n",
    "        # Le pointeur rencontre des caractères, attribution de token\n",
    "        # ---\n",
    "\n",
    "        # Rencontre de chiffre/nombre\n",
    "        if self.is_number(self.text[self.pointer_pos]): # parse full number\n",
    "\n",
    "            # Création du nombre\n",
    "            current_number = ''\n",
    "\n",
    "            # Ajout de chaque chiffre du nombre caractère par caractère          \n",
    "            while not self.test_eof() and self.is_number(self.text[self.pointer_pos]):\n",
    "                current_number += self.text[self.pointer_pos]\n",
    "                self.pointer_pos+=1\n",
    "                \n",
    "            # Le nombre est complet, création de token\n",
    "            self.current_token = Token(\"tok_const\",(self.current_line,self.current_col),token_value=int(current_number))\n",
    "            \n",
    "            # Le token a été trouvé, arret de la méthode\n",
    "            return\n",
    "\n",
    "        # Rencontre de lettre/mot\n",
    "        if self.is_letter(self.text[self.pointer_pos]):\n",
    "\n",
    "            # Création du mot\n",
    "            current_word = ''\n",
    "\n",
    "            # Tant que le pointeur n'est pas à la fin du code et qu'il rencontre des caratères\n",
    "            while not self.test_eof() and self.is_alpha_num(self.text[self.pointer_pos]): # ici on utilise is_alpha_num car un identifiant ne peut pas commencer par une lettre mais il peut en avoir ensuite\n",
    "                # Ajout de la lettre ou du chiffre au mot\n",
    "                current_word += self.text[self.pointer_pos]\n",
    "\n",
    "                self.pointer_pos+=1\n",
    "\n",
    "            # Vérification de la présence du mot dans les mots clefs réservés\n",
    "            if current_word in self.keywords:\n",
    "                # Création du token particulier réservé au mot-clef\n",
    "                self.current_token = Token(\"tok_\"+current_word,(self.current_line,self.current_col),token_string=current_word)\n",
    "            else:\n",
    "                # Création d'un token d'identification\n",
    "                self.current_token = Token(\"tok_ident\",(self.current_line,self.current_col),token_string=current_word)\n",
    "            \n",
    "            # Token trouvé, arret de la méthode\n",
    "            return\n",
    "        \n",
    "        # ---\n",
    "        # Si le caractère rencontré est un token en lui-même, renvoi du token correspondant\n",
    "        # ---\n",
    "        match self.text[self.pointer_pos]:\n",
    "            case '+':\n",
    "                self.current_token = Token(\"tok_+\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '-':\n",
    "                self.current_token = Token(\"tok_-\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '*':\n",
    "                self.current_token = Token(\"tok_*\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '/':\n",
    "                self.current_token = Token(\"tok_/\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '%':\n",
    "                self.current_token = Token(\"tok_%\",(self.current_line,self.current_col))\n",
    "            \n",
    "            case '&':\n",
    "                # Distinction entre '&' et '&&'\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '&':\n",
    "                    self.current_token = Token(\"tok_&&\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    self.current_token = Token(\"tok_&\",(self.current_line,self.current_col))\n",
    "            \n",
    "            case '|':\n",
    "                # Le token '|' n'existe pas, vérification de la présence de '||'\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '|':\n",
    "                    self.current_token = Token(\"tok_||\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    raise ValueError(f\"Token | unkown at pos (line = {self.current_line} col = {self.current_col}) did you mean \\\"||\\\" ?\")\n",
    "            \n",
    "            case '!':\n",
    "                # Distinction entre '!' et '!='\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '=':\n",
    "                    self.current_token = Token(\"tok_!=\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    self.current_token = Token(\"tok_!\",(self.current_line,self.current_col))\n",
    "            \n",
    "            case '=':\n",
    "                # Distinction entre '=' et '=='\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '=':\n",
    "                    self.current_token = Token(\"tok_==\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    self.current_token = Token(\"tok_=\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '<':\n",
    "                # Distinction entre '<' et '<='\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '=':\n",
    "                    self.current_token = Token(\"tok_<=\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    self.current_token = Token(\"tok_<\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '>':\n",
    "                # Distinction entre '>' et '>='\n",
    "                if self.pointer_pos + 1 < len(self.text) and self.text[self.pointer_pos+1] == '=':\n",
    "                    self.current_token = Token(\"tok_>=\",(self.current_line,self.current_col))\n",
    "                    self.pointer_pos+=1\n",
    "                    self.current_col+=1\n",
    "                else:\n",
    "                    self.current_token = Token(\"tok_<\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '(':\n",
    "                self.current_token = Token(\"tok_(\",(self.current_line,self.current_col))\n",
    "\n",
    "            case ')':\n",
    "                self.current_token = Token(\"tok_)\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '[':\n",
    "                self.current_token = Token(\"tok_[\",(self.current_line,self.current_col))\n",
    "\n",
    "            case ']':\n",
    "                self.current_token = Token(\"tok_]\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '{':\n",
    "                self.current_token = Token(\"tok_{\",(self.current_line,self.current_col))\n",
    "\n",
    "            case '}':\n",
    "                self.current_token = Token(\"tok_}\",(self.current_line,self.current_col))\n",
    "\n",
    "            case ';':\n",
    "                self.current_token = Token(\"tok_;\",(self.current_line,self.current_col))\n",
    "\n",
    "            case ',':\n",
    "                self.current_token = Token(\"tok_,\",(self.current_line,self.current_col))\n",
    "\n",
    "            case other: # si token inconnue, on envoie une erreur\n",
    "                raise ValueError(f\"Token {other} unkown at pos (line = {self.current_line} col = {self.current_col})\")\n",
    "        self.pointer_pos+=1\n",
    "    \n",
    "    def accept(self,token_type:str):\n",
    "        \"\"\"\n",
    "        Vérification du type du current_token par rapport au type fourni en argument.\n",
    "        Si le type du current_token n'est pas le bon, un message d'erreur est envoyé\n",
    "\n",
    "        @params\n",
    "        Entrée : token_type, type string, type du token \n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        assert type(token_type)==str, \"Mauvais type d'argument token_type (str attendu)\"\n",
    "\n",
    "        if not self.check(token_type):\n",
    "            line, col = self.current_token.token_pos\n",
    "            raise ValueError(f\"wrong token at pos ({line = } {col = }) expected a token of type {token_type}\")\n",
    "\n",
    "    def check(self,token_type:str):\n",
    "        \"\"\"\n",
    "        Vérification du type du current_token par rapport au type fourni en argument.\n",
    "        Si le type est le bon, appel à next_token() pour trouver la token d'avant,\n",
    "        renvoi de 'True'\n",
    "        Si le type du current_token n'est pas le bon, 'False' est renvoyé\n",
    "\n",
    "        @params\n",
    "        Entrée : token_type, type string, type du token \n",
    "        Sortie : Boolean, résultat de la comparaison\n",
    "        \"\"\"\n",
    "        assert type(token_type)==str, \"Mauvais type d'argument token_type (str attendu)\"\n",
    "\n",
    "        if self.current_token.token_type == token_type:\n",
    "            self.next_token()\n",
    "            return True\n",
    "        return False\n",
    "                \n",
    "class Parser:\n",
    "\n",
    "    def __init__(self,lexer:Lexer):\n",
    "        \"\"\" \n",
    "        Classe récupérant les tokens un par un et les transforme en noeud pour créer l'arbre représentant le code\n",
    "\n",
    "        @params:\n",
    "        Entrée : lexer, type Lexer, objet d'analyse du code pour la transformation en token\n",
    "        Sortie : None\n",
    "        \"\"\"\n",
    "        self.lexer = lexer\n",
    "        \n",
    "    \n",
    "    def next_tree(self):\n",
    "        \"\"\" \n",
    "        Méthode renvoyant un noeud représentant la prochaine expression\n",
    "\n",
    "        @params:\n",
    "        Entrée : None\n",
    "        Sortie : Node\n",
    "        \"\"\"\n",
    "        return self.get_expression()\n",
    "\n",
    "\n",
    "    def get_expression(self, prio: int = 0) -> Node:\n",
    "        \"\"\" \n",
    "        Méthode renvoyant un noeud représentant une expression complète\n",
    "\n",
    "        @params:\n",
    "        Entrée : prio, type : Int, priorité comparée\n",
    "        Sortie : Node, type : Node, Noeud opérateur créé à l'issue de la fonction\n",
    "        \"\"\"\n",
    "        assert type(prio)==int, \"Mauvais type de priorité (int attendu)\"\n",
    "\n",
    "        # Récupération du préfixe\n",
    "        first_part = self.get_prefix()\n",
    "\n",
    "        # Vérification de la priorité du token regardé (global) par rapport à l'argument prio\n",
    "        while check_op_prio(self.lexer.current_token.token_type, prio):\n",
    "\n",
    "            # Tant que le token vérifé a une priorité plus grande que l'argument\n",
    "            op_token = self.lexer.current_token\n",
    "\n",
    "            # Le pointeur avance et prend le token suivant\n",
    "            self.lexer.next_token()\n",
    "\n",
    "            # Récupération de la seconde partie de l'expression suivant l'opérateur\n",
    "            second_part = self.get_expression(OP[op_token.token_type][\"parg\"])\n",
    "\n",
    "            # Création d'un noeud opérateur avec les deux parties de l'expression\n",
    "            first_part = Node(OP[op_token.token_type][\"node_type\"],node_pos=op_token.token_pos,children=[first_part,second_part])\n",
    "        \n",
    "        # Renvoi de la première partie de l'expression ou du noeud créé en fonction des priorités\n",
    "        return first_part\n",
    "\n",
    "    def get_suffix(self) -> Node:\n",
    "        \"\"\" \n",
    "        Méthode renvoyant un noeud opérateur suffixe (appel de fonction, indexation) avec comme enfant le reste de l'expression\n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : Node\n",
    "        \"\"\"\n",
    "\n",
    "        return self.get_atom()\n",
    "\n",
    "    def get_prefix(self) -> Node:\n",
    "        \"\"\" \n",
    "        Méthode renvoyant un noeud opérateur préfixe (!, -, +, *, &) avec comme enfant le reste de l'expression\n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : Node\n",
    "        \"\"\"\n",
    "\n",
    "        # Parcours en fonction du préfixe\n",
    "        if self.lexer.check(\"tok_!\"):\n",
    "\n",
    "            # Récupération du dernier token rencontré ( tok_! )\n",
    "            token_not = self.lexer.last_token\n",
    "            \n",
    "            # Récupération du reste de l'expression en se rappelant elle-même\n",
    "            intern_prefix = self.get_prefix()\n",
    "\n",
    "            # Création d'un noeud correspondant au token\n",
    "            node_not = Node(\"nd_not\",node_pos=token_not.token_pos,children=[intern_prefix])\n",
    "\n",
    "            # Renvoi du noeud\n",
    "            return node_not\n",
    "        \n",
    "        elif self.lexer.check(\"tok_-\"):\n",
    "\n",
    "            # Récupération du dernier token rencontré ( tok_- )\n",
    "            token_neg = self.lexer.last_token\n",
    "\n",
    "            # Récupération du reste de l'expression en se rappelant elle-même\n",
    "            intern_prefix = self.get_prefix()\n",
    "\n",
    "            # Création d'un noeud correspondant au token\n",
    "            node_neg = Node(\"nd_neg\",node_pos=token_neg.token_pos,children=[intern_prefix])\n",
    "\n",
    "            # Renvoi du noeud\n",
    "            return node_neg\n",
    "        \n",
    "        elif self.lexer.check(\"tok_+\"):\n",
    "            # Préfix + inutile (comme dans \"+5\", suppression du \"+\" inutile)\n",
    "            return self.get_prefix()\n",
    "        \n",
    "        else:\n",
    "            # Lorsqu'on rencontre quelque chose de différent des préfixes définis, renvoi en tant que suffixe\n",
    "            return self.get_suffix()\n",
    "\n",
    "    def get_atom(self) -> Node:\n",
    "        \"\"\" \n",
    "        Méthode renvoyant un noeud atome (constante numérique ou une expression entre parenthèse) \n",
    "\n",
    "        @params\n",
    "        Entrée : None\n",
    "        Sortie : Node\n",
    "        \"\"\"\n",
    "\n",
    "        # Parcours en fonction du token rencontré\n",
    "        if self.lexer.check(\"tok_const\"):\n",
    "\n",
    "            # Récupération du dernier token ( tok_const )\n",
    "            token = self.lexer.last_token\n",
    "\n",
    "            # Renvoi du noeud correspondant au token\n",
    "            return Node(\"nd_const\",node_pos=token.token_pos,node_value=token.token_value)\n",
    "\n",
    "        elif self.lexer.check(\"tok_(\"):\n",
    "\n",
    "            # Récupération de l'expression parenthésée\n",
    "            expression = self.get_expression()\n",
    "\n",
    "            # Vérification de la fermeture de l'expression par une parenthèse fermante\n",
    "            self.lexer.accept(\"tok_)\")\n",
    "\n",
    "            # Renvoi de l'expression\n",
    "            return expression\n",
    "        \n",
    "        else:\n",
    "            print(self.lexer.current_token)\n",
    "            # Token non accepté dans la grammaire régissant ce modèle atome, renvoi d'une erreur\n",
    "            raise ValueError(f\"error at pos {self.lexer.current_token.token_pos}, expected const or expression\")\n",
    "\n",
    "    \n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self,parser:Parser):\n",
    "        self.parser = parser\n",
    "    \n",
    "    def next_tree(self):\n",
    "        return self.parser.next_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a009a",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f65d269",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'code.c'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdbg\u001b[39m\u001b[33m\"\u001b[39m,file=file)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mhalt\u001b[39m\u001b[33m\"\u001b[39m,file=file)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcode.c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         code = f.read()\n\u001b[32m      5\u001b[39m     lexer = Lexer(code)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'code.c'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with open(\"code.c\", 'r') as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    lexer = Lexer(code)\n",
    "    lexer.next_token()\n",
    "    parser = Parser(lexer)\n",
    "    optimizer = Optimizer(parser)\n",
    "\n",
    "    with open(\"msm/prg.asm\",'w') as file:\n",
    "\n",
    "        \n",
    "\n",
    "        print(\".start\",file=file)\n",
    "\n",
    "        while(lexer.current_token.token_type != \"tok_eof\"):\n",
    "            gencode(optimizer,file=file)\n",
    "\n",
    "        print(\"dbg\",file=file)\n",
    "        print(\"halt\",file=file)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0bf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_prio = {'6':['*','/','%'],\n",
    "             '5':['+','-'],\n",
    "             '4':['==','!=','<','<=','>','>='],\n",
    "             '3':['&&'],\n",
    "             '2':['||'],\n",
    "             '1':['=']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c20f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "unit_to_find = '+'\n",
    "\n",
    "for (key,item) in dict_prio.items():\n",
    "    if '+' in item:\n",
    "        print(int(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59d9d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[value : 56, pos : (0, 0)]\n",
      "[value : 32, pos : (0, 2)]\n",
      "\n",
      "[type : tok_+, childrens : ['[value : 56, pos : (0, 0)]', '[value : 32, pos : (0, 2)]'], pos : (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "noeud1 = Node(\"tok_const\", (0,0), 56)\n",
    "noeud2 = Node(\"tok_const\", (0,2), 32)\n",
    "noeud3 = Node(\"tok_+\", (0,1), children=[noeud1,noeud2])\n",
    "\n",
    "print(noeud1)\n",
    "print(noeud2)\n",
    "print(noeud3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29bec466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0\n",
      "False\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(1+3*6+3-(1*(4-2)/2)%8)\n",
    "print(1<2 and 9!=9)\n",
    "print(((1 and 1) + 19) or (8 == 1 - 12/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9034fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'test' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     test+=\u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(test)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mt\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mt\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtest\u001b[49m+=\u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(test)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'test' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "test = 5\n",
    "\n",
    "def t():\n",
    "    test+=1\n",
    "    print(test)\n",
    "\n",
    "t()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f902d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
